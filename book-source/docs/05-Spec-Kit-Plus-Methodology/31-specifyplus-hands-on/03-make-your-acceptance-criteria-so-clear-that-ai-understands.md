---
title: "Make Your Acceptance Criteria So Clear That AI Understands"
chapter: 31
lesson: 3
duration: "2 hours"
skills:
  - name: "Acceptance Criteria Writing"
    proficiency: "B1"
    category: "Technical"
  - name: "Testability Design"
    proficiency: "B1"
    category: "Technical"
  - name: "Machine-Executable Specifications"
    proficiency: "B1"
    category: "Technical"
learning_objectives:
  - "Write acceptance criteria that are unambiguous and machine-executable (B1)"
  - "Structure criteria for AI comprehension without human context (B1)"
  - "Design testable conditions that verify implementation correctness (B1)"
---

# Make Your Acceptance Criteria So Clear That AI Understands

## What You're About to Discover

The most important discovery in specification-driven development is this: **vague acceptance criteria produce vague code. Clear acceptance criteria produce usable code.**

You're going to experience this directly. You'll write vague criteria, ask your companion to build from them, see what goes wrong, then refine the criteria. With each refinement, the code gets better. You'll understand—not intellectually, but *experientially*—why clarity matters.

---

## Part 1: Write Vague Acceptance Criteria

Let's start with a grading system component: the ability to generate feedback.

Here's how you'd *naturally* describe it (vaguely):

```
The system should give feedback to students.
It should be accurate.
It should be helpful.
It should work quickly.
```

**Open your companion and ask it to build based on these vague criteria:**

```
I want a function that gives feedback to students
based on their assignment submissions.

Requirements:
- The system should give feedback to students
- It should be accurate
- It should be helpful
- It should work quickly

Write me Python code that does this.
Can you use some realistic sample data?
```

Your companion will generate something like this:

```python
def generate_feedback(submission: str) -> str:
    """Generate feedback for a student submission."""
    if len(submission) < 100:
        return "Your submission is too short. Write more."
    elif len(submission) > 2000:
        return "Your submission is too long. Be more concise."
    else:
        return "Good work. Keep improving."
```

**Look at what you got**: The code works. It runs. It gives feedback.

But ask yourself:

- **Is this accurate?** Accurate according to what criteria?
- **Is this helpful?** It's generic—any submission between 100-2000 words gets "Good work."
- **Is this quick?** Yes, it's very quick. But is speed the right metric? (Probably not for grading.)

Your companion followed your vague spec to the letter. And the result is useless.

---

## Part 2: Analyze What Went Wrong

**Ask your companion:**

```
Look at that feedback function. Why is it not very useful?
What information would I need to give you to make it better?
```

Your companion will say something like:

> "The feedback is too generic. I don't know:
> - What criteria should be evaluated (grammar? clarity? logic?)
> - How should those criteria be scored?
> - Should feedback be different for essays vs. code?
> - What does 'helpful' actually mean to you?
> - Should feedback be generated by AI, or from templates?
> - What's the format of feedback? One sentence? Paragraph?
>
> I just used the most basic heuristics I could think of
> because I didn't know what you actually wanted."

This is the key insight: **Your companion isn't being lazy. You were being vague.**

---

## Part 3: Refine Using the SMART Framework

Now you're going to apply the **SMART criteria** framework to fix this. SMART stands for:

- **S (Specific)**: What exactly should this do? (Not "accurate," but specifically what?)
- **M (Measurable)**: How would you test it? (Not "helpful," but what would we measure?)
- **A (Achievable)**: Is this realistic? (Not requiring AI telepathy)
- **R (Relevant)**: Does this match the real problem?
- **T (Time-bound)**: Should this have a deadline or performance limit?

Let's rebuild the acceptance criteria:

**Vague**:
```
The system should give feedback to students.
It should be accurate.
It should be helpful.
It should work quickly.
```

**SMART**:
```
### Feedback Generation (For Essays)

**Specific Requirement**:
The system generates feedback by evaluating each essay against
a teacher-defined rubric with 3-5 criteria.

**Measurable Criteria**:
- Feedback is 50-200 words per submission
- Feedback addresses at least 2 of the rubric criteria explicitly
- Feedback includes 1 specific example from the submission
- Response time is < 3 seconds per submission

**Achievable**:
Uses template-based feedback (not AI-generated) combined with
basic text analysis to identify problematic areas.

**Relevant**:
Teachers need specific, actionable feedback. Students need to know
what they did well and what to improve.

**Time-bound**:
Processing budget: 3 seconds per essay (includes API calls if any).
```

---

## Part 4: Test the Refined Criteria

**Now ask your companion to build from the SMART criteria:**

```
I want you to write a feedback function with these specifications:

### Feedback Generation (For Essays)

Specific Requirement:
The system generates feedback by evaluating each essay against
a teacher-defined rubric with 3-5 criteria.

Measurable Criteria:
- Feedback is 50-200 words per submission
- Feedback addresses at least 2 of the rubric criteria explicitly
- Feedback includes 1 specific example from the submission
- Response time is < 3 seconds per submission

Write Python code that does this. Use this sample essay:

Essay: "Climate change is important. We should do something about it.
       Trees are good. Recycling helps. We need to act now."

Rubric:
- Argument: Is there a clear thesis?
- Evidence: Are claims supported by examples?
- Clarity: Is the writing clear and organized?"

Your companion will produce something much better:

```python
from dataclasses import dataclass
from typing import Optional

@dataclass
class RubricCriterion:
    name: str
    description: str

def generate_feedback(
    essay: str,
    rubric: list[RubricCriterion],
    max_length: int = 200
) -> str:
    """Generate feedback evaluating essay against rubric criteria."""

    feedback_parts = []

    # Check for thesis (Argument criterion)
    if len(essay.split('.')) < 3:
        feedback_parts.append(
            "Your argument needs a clearer thesis. "
            "Currently, sentences like 'Climate change is important' "
            "state the topic but don't defend a specific position."
        )

    # Check for evidence
    if 'because' not in essay.lower() and 'example' not in essay.lower():
        feedback_parts.append(
            "Add evidence for your claims. You wrote 'Trees are good' "
            "but didn't explain why or provide examples."
        )

    # Compile feedback
    feedback = " ".join(feedback_parts)

    # Trim to word limit
    words = feedback.split()
    if len(words) > max_length:
        feedback = " ".join(words[:max_length]) + "..."

    return feedback
```

**Compare the two**:

| Criterion | Vague Version | SMART Version |
|-----------|---------------|---------------|
| Feedback length | Unpredictable | 50-200 words (specified) |
| Criteria evaluated | Maybe 1? | Multiple rubric criteria |
| Specificity | Generic "Good work" | References specific text |
| Response time | Unknown | < 3 seconds |
| Testability | Can't verify if "accurate" | Can test word count, criteria coverage |

The SMART version is **infinitely more useful** because the criteria were infinitely more specific.

---

## Part 5: Extract Your Acceptance Criteria Template

Based on this exercise, create a template for writing acceptance criteria:

```markdown
## Acceptance Criteria Template

### [Feature Name]

**Specific Requirement**:
[What exactly should this do? Avoid words like "good," "easy," "accurate."]

**Measurable Criteria**:
- [Something testable with numbers or specific outcomes]
- [Another measurable thing]
- [One more]

**Achievable**:
[Is this realistic? What tools/knowledge does this require?]

**Relevant**:
[Why does this matter? How does this help the user?]

**Time-bound**:
[Performance budget? Deadline? Any time constraints?]
```

Save this template. You'll use it for every feature moving forward.

---

## Part 6: Build Your Grading System Acceptance Criteria

Now you're going to write SMART acceptance criteria for the core features of your grading system.

**Feature 1: Rubric Definition**

```markdown
### Rubric Definition

Specific Requirement:
Teachers can create a rubric with 3-5 criteria. Each criterion
has a name and a description (0-100 words). Criteria are stored
and retrievable.

Measurable Criteria:
- Each rubric supports 3-5 criteria (no more, no less)
- Each criterion has a name (1-50 characters)
- Each criterion has a description (0-100 words)
- Rubrics can be saved and loaded
- Editing a rubric updates all future uses of it

Achievable:
Python dict or dataclass to store criteria. No database required.

Relevant:
Teachers need clear, reusable grading standards.

Time-bound:
Creating a rubric should take < 30 seconds.
```

**Feature 2: Assignment Upload**

```markdown
### Assignment Upload

Specific Requirement:
Students can upload essay files (.txt) and code files (.py).
Files are validated for format and size. Uploads are stored
with metadata (student name, date, filename).

Measurable Criteria:
- Accepts .txt files up to 10,000 words
- Accepts .py files up to 1,000 lines
- Validates file format before accepting
- Stores original filename and upload timestamp
- Rejects files that exceed size limits with clear error message

Achievable:
File upload with basic validation. No cloud storage needed initially.

Relevant:
Teachers need to collect student work in multiple formats.

Time-bound:
File upload completes in < 2 seconds.
```

**Feature 3: Grade Storage**

```markdown
### Grade Storage

Specific Requirement:
System stores grades with associated metadata and allows retrieval
of historical grades.

Measurable Criteria:
- Each grade includes: student name, assignment ID, score (0-100),
  feedback text, rubric used, date graded
- Grades can be retrieved by student name
- Grades can be retrieved by assignment ID
- Grades persist across sessions
- Historical grades cannot be deleted (audit trail)

Achievable:
Python dict persisted to JSON file. No database required.

Relevant:
Teachers need to track student progress over time.

Time-bound:
Grade retrieval < 1 second. Grade storage < 2 seconds.
```

---

## Part 7: Why SMART Criteria Matter

By writing these SMART criteria, you've done something powerful:

1. **You've made it impossible to misinterpret what you want.** No AI, no human, can read "Each criterion has a name (1-50 characters)" and misunderstand it.

2. **You've made it testable.** Someone can check: "Does this rubric have 3-5 criteria?" Yes/no. "Does each criterion have a description?" Yes/no.

3. **You've prevented scope creep.** Your spec says "no database." So any suggestion to add a database violates the spec.

4. **You've made it buildable.** An AI (or a human) can look at these criteria and know exactly what to build.

---

## Try With AI

You're going to practice writing SMART acceptance criteria by refining criteria your companion suggests.

### Prompt 1: Get Vague Criteria From Your Companion
```
I'm building a grading system. What are some core features
I should include? Just list them with basic descriptions—don't
worry about being specific yet.
```

**Expected outcome**: Your companion gives you 5-8 vague feature ideas.

### Prompt 2: Pick One and SMART It
```
Let's take this feature: [feature from Prompt 1]

Help me write SMART acceptance criteria for it. Here's the template:

- Specific Requirement: [exactly what it does]
- Measurable Criteria: [testable, quantifiable outcomes]
- Achievable: [is this realistic?]
- Relevant: [why does this matter?]
- Time-bound: [performance budgets?]

For [feature], what would good criteria look like?
```

**Expected outcome**: Your companion helps you refine vague ideas into specific, testable criteria. You might go back and forth—that's the point.

### Prompt 3: Validate Your Criteria
```
I've written SMART criteria for [feature]. Here they are:

[paste your criteria]

Can you tell me:
1. Are these specific enough that you could build from them?
2. What's still ambiguous?
3. What would you need to know to start building?
```

**Expected outcome**: Your companion points out gaps or ambiguities. You refine them further. This iterative process is the real learning.

---

## Safety & Next Steps

You've now mastered the most critical skill in specification-driven development: **writing clear, testable acceptance criteria.**

This skill will make every AI interaction better. Clearer criteria → better code. Better code → less iteration. Less iteration → faster projects.

In the next lesson, you're going to take your partial spec and run it through `/sp.specify`, SpecifyPlus's refinement tool. This will show you what gaps exist in your spec and how to fill them.

**Next**: You'll run `/sp.specify` and discover that your clear thinking has set you up to get even clearer feedback from the tool.
